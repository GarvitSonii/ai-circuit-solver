{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bfa4c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86a75adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60df7e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f6406a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=8192, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=15, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=15):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)  # 3 channels (RGB)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(32*16*16, 128)  # 64x64 input → after 2 pool layers → 16x16\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN(num_classes=15)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e09305b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\garvi\\Desktop\\ai_circuit_solver\\notebooks\n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# 1. Check current working directory\n",
    "# -------------------\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91c5ab6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\garvi\\Desktop\\ai_circuit_solver\\split_data\n",
      "Train folder exists: True\n",
      "Validation folder exists: True\n",
      "Test folder exists: True\n"
     ]
    }
   ],
   "source": [
    "# Parent folder of notebooks\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))  # go up one level\n",
    "data_dir = os.path.join(base_dir, 'split_data')\n",
    "\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "val_dir   = os.path.join(data_dir, 'val')\n",
    "test_dir  = os.path.join(data_dir, 'test')\n",
    "print(data_dir)\n",
    "print(\"Train folder exists:\", os.path.exists(train_dir))\n",
    "print(\"Validation folder exists:\", os.path.exists(val_dir))\n",
    "print(\"Test folder exists:\", os.path.exists(test_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d805d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# 3. Transformations\n",
    "# -------------------\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "711fb717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# 4. Datasets\n",
    "# -------------------\n",
    "train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "val_data   = datasets.ImageFolder(val_dir, transform=val_test_transform)\n",
    "test_data  = datasets.ImageFolder(test_dir, transform=val_test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31dc4b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# 5. DataLoaders\n",
    "# -------------------\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "val_loader   = DataLoader(val_data, batch_size=16, shuffle=False)\n",
    "test_loader  = DataLoader(test_data, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba0ed642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Ammeter', 'ac_src', 'battery', 'cap', 'curr_src', 'dc_volt_src_1', 'dc_volt_src_2', 'dep_curr_src', 'dep_volt', 'diode', 'gnd_1', 'gnd_2', 'inductor', 'resistor', 'voltmeter']\n",
      "Train samples: 2062\n",
      "Validation samples: 438\n",
      "Test samples: 452\n"
     ]
    }
   ],
   "source": [
    "classes = train_data.classes\n",
    "print(\"Classes:\", classes)\n",
    "print(\"Train samples:\", len(train_data))\n",
    "print(\"Validation samples:\", len(val_data))\n",
    "print(\"Test samples:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b5467aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94beace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebd62f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.9234\n",
      "Validation Accuracy: 57.99%\n",
      "\n",
      "Epoch 2/5, Loss: 1.0042\n",
      "Validation Accuracy: 69.18%\n",
      "\n",
      "Epoch 3/5, Loss: 0.7134\n",
      "Validation Accuracy: 67.35%\n",
      "\n",
      "Epoch 4/5, Loss: 0.5674\n",
      "Validation Accuracy: 75.34%\n",
      "\n",
      "Epoch 5/5, Loss: 0.4324\n",
      "Validation Accuracy: 75.34%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5  # start small\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_acc = correct / total\n",
    "    print(f\"Validation Accuracy: {val_acc*100:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "988a5818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 73.45%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {correct/total*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a341ece9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: resistor\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "path=os.path.join(test_dir, 'resistor/5.png')\n",
    "img = Image.open(path).convert('RGB')\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "img = transform(img).unsqueeze(0)  # add batch dimension\n",
    "img = img.to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(img)\n",
    "    _, predicted_class = torch.max(output, 1)\n",
    "\n",
    "\n",
    "print(\"Predicted class:\", train_data.classes[predicted_class.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9176a5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
